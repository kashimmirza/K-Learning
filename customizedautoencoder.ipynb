{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashimmirza/K-Learning/blob/master/customizedautoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c484ce38",
      "metadata": {
        "id": "c484ce38"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871ec43e",
      "metadata": {
        "id": "871ec43e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4769f57d",
      "metadata": {
        "id": "4769f57d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import random \n",
        "from torchvision.datasets import ImageFolder\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from skimage.io import imread, imsave\n",
        "import skimage\n",
        "from PIL import ImageFile\n",
        "from PIL import Image\n",
        "from torchsummary import summary\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f92fc8c",
      "metadata": {
        "id": "1f92fc8c"
      },
      "outputs": [],
      "source": [
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "########## Mean and std are calculated from the train dataset\n",
        "normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
        "                                     std=[0.33165374, 0.33165374, 0.33165374])\n",
        "train_transformer = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomRotation(90),\n",
        "    # random brightness and random contrast\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transformer = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.CenterCrop(224),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f77e6302",
      "metadata": {
        "id": "f77e6302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "e2e8309d-c02f-4970-b442-42b9d858ac76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9236a0de02b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtxt_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCovidCTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_COVID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_NonCOVID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \"\"\"\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "batchsize=4\n",
        "def read_txt(txt_path):\n",
        "    with open(txt_path) as f:\n",
        "        lines = f.readlines()\n",
        "    txt_data = [line.strip() for line in lines]\n",
        "    return txt_data\n",
        "\n",
        "class CovidCTDataset(Dataset):\n",
        "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_path (string): Path to the txt file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        File structure:\n",
        "        - root_dir\n",
        "            - CT_COVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "            - CT_NonCOVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
        "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
        "        self.num_cls = len(self.classes)\n",
        "        self.img_list = []\n",
        "        for c in range(self.num_cls):\n",
        "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
        "            self.img_list += cls_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.img_list[idx][0]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {'img': image,\n",
        "                  'label': int(self.img_list[idx][1])}\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39518ca2",
      "metadata": {
        "id": "39518ca2",
        "outputId": "e53c9e0a-7526-4bf2-8304-d00aa3c70138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-109b8645ab74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mtxt_COVID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'E:/pythonlatestfolder1/datasets/new_data/newtxt/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mtxt_NonCOVID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'E:/pythonlatestfolder1/datasets/old_data/oldtxt/trainctnoncovid.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                               transform= train_transformer)\n\u001b[0m\u001b[1;32m      6\u001b[0m     valset = CovidCTDataset(root_dir='E:/pythonlatestfolder1/datasets/new_data/4.4image',\n\u001b[1;32m      7\u001b[0m                               \u001b[0mtxt_COVID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'E:/pythonlatestfolder1/datasets/new_data/newtxt/val.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9236a0de02b7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, txt_COVID, txt_NonCOVID, transform)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mcls_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcls_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9236a0de02b7>\u001b[0m in \u001b[0;36mread_txt\u001b[0;34m(txt_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtxt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/pythonlatestfolder1/datasets/new_data/newtxt/train.txt'"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    trainset = CovidCTDataset(root_dir='E:/pythonlatestfolder1/datasets/new_data/4.4image',\n",
        "                              txt_COVID='E:/pythonlatestfolder1/datasets/new_data/newtxt/train.txt',\n",
        "                              txt_NonCOVID='E:/pythonlatestfolder1/datasets/old_data/oldtxt/trainctnoncovid.txt',\n",
        "                              transform= train_transformer)\n",
        "    valset = CovidCTDataset(root_dir='E:/pythonlatestfolder1/datasets/new_data/4.4image',\n",
        "                              txt_COVID='E:/pythonlatestfolder1/datasets/new_data/newtxt/val.txt',\n",
        "                              txt_NonCOVID='E:/pythonlatestfolder1/datasets/old_data/oldtxt/valctnoncovid.txt',\n",
        "                              transform= val_transformer)\n",
        "    testset = CovidCTDataset(root_dir='E:/pythonlatestfolder1/datasets/new_data/4.4image',\n",
        "                              txt_COVID='E:/pythonlatestfolder1/datasets/new_data/newtxt/test.txt',\n",
        "                              txt_NonCOVID='E:/pythonlatestfolder1/datasets/old_data/oldtxt/testct_noncovid.txt',\n",
        "                              transform= val_transformer)\n",
        "    print(trainset.__len__())\n",
        "    print(valset.__len__())\n",
        "    print(testset.__len__())\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dcc59a",
      "metadata": {
        "id": "64dcc59a",
        "outputId": "e4c8de29-903d-4c48-df55-3e80a6909260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b54e60c82163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "for batch_index, batch_samples in enumerate(train_loader):      \n",
        "        data, target = batch_samples['img'], batch_samples['label']\n",
        "skimage.io.imshow(data[0,1,:,:].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f2c5f7",
      "metadata": {
        "id": "46f2c5f7",
        "outputId": "2a2b103d-2af5-4ec4-8c05-eb3247bc7732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9a5458",
      "metadata": {
        "id": "ea9a5458",
        "outputId": "5f5b4417-2f78-430a-b4ac-3ebdeed24b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-a8804d162bee>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    print('encoder :'encoder)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define the NN architecture\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding =0)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
        "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        # add hidden layers with relu activation function\n",
        "        # and maxpooling after\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        # add second hidden layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)# compressed representation\n",
        "        encoder = x;\n",
        "        m = nn.flatten()\n",
        "        encoder = m(encoder)\n",
        "        print('encoder :'encoder)\n",
        "        \n",
        "        print(x)\n",
        "        \n",
        "        ## decode ##\n",
        "        # add transpose conv layers, with relu activation function\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        # output layer (with sigmoid for scaling from 0 to 1)\n",
        "        x = F.sigmoid(self.t_conv2(x))\n",
        "                \n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = ConvAutoencoder()\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "summary(model,(3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c620bde",
      "metadata": {
        "id": "7c620bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "25b0c023-03dd-405b-f042-370e2269ce1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvAutoencoder(\n",
            "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (t_conv2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (t_conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6bd2b7f1c5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6bd2b7f1c5bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# add hidden layers with relu activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# and maxpooling after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# add second hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 64, 3, 3], expected input[2, 3, 224, 224] to have 64 channels, but got 3 channels instead"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define the NN architecture\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
        "        self.conv1 = nn.Conv2d(3, 512, 3, padding =0,stride =2)\n",
        "        self.conv1 = nn.Conv2d(512, 64, 3, padding =0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(64, 256, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(256, 512, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(512, 512, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(512, 512, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 512, 3, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 512, 3, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 256, 3, stride=2)\n",
        "        self.t_conv1 = nn.Conv2d(3, 64, 3, padding =0,stride =2)\n",
        "        self.conv1 = nn.Conv2d(64, 64, 3, padding =0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(64, 256, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        # add hidden layers with relu activation function\n",
        "        # and maxpooling after\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        # add second hidden layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)# compressed representation\n",
        "        encoder = x;\n",
        "        \n",
        "        print(x)\n",
        "        \n",
        "        ## decode ##\n",
        "        # add transpose conv layers, with relu activation function\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        # output layer (with sigmoid for scaling from 0 to 1)\n",
        "        x = F.sigmoid(self.t_conv2(x))\n",
        "                \n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = ConvAutoencoder()\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "summary(model,(3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b7c61152",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7c61152",
        "outputId": "86cbdc56-ced6-49a9-cb51-2e3f0fba026e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier Linear(in_features=6144, out_features=2, bias=True)\n",
            "ConvAutoencoder(\n",
            "  (conv1): Conv2d(3, 512, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv3): Conv2d(256, 128, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (conv4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv5): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (classifier): Linear(in_features=6144, out_features=2, bias=True)\n",
            "  (t_conv1): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (t_conv2): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (t_conv3): ConvTranspose2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (t_conv4): ConvTranspose2d(256, 512, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (t_conv5): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(1, 1))\n",
            ")\n",
            "tensor([[[[0.0120, 0.0118, 0.0113, 0.0117, 0.0120],\n",
            "          [0.0126, 0.0127, 0.0117, 0.0110, 0.0132],\n",
            "          [0.0131, 0.0145, 0.0122, 0.0124, 0.0129],\n",
            "          [0.0121, 0.0128, 0.0133, 0.0134, 0.0123],\n",
            "          [0.0120, 0.0134, 0.0103, 0.0133, 0.0137]],\n",
            "\n",
            "         [[0.0335, 0.0341, 0.0352, 0.0326, 0.0350],\n",
            "          [0.0357, 0.0340, 0.0340, 0.0342, 0.0334],\n",
            "          [0.0334, 0.0337, 0.0369, 0.0351, 0.0347],\n",
            "          [0.0328, 0.0334, 0.0315, 0.0340, 0.0343],\n",
            "          [0.0328, 0.0362, 0.0343, 0.0347, 0.0336]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0122, 0.0119, 0.0115, 0.0122, 0.0111],\n",
            "          [0.0107, 0.0124, 0.0114, 0.0111, 0.0113],\n",
            "          [0.0118, 0.0134, 0.0145, 0.0126, 0.0123],\n",
            "          [0.0119, 0.0112, 0.0133, 0.0105, 0.0137],\n",
            "          [0.0131, 0.0122, 0.0102, 0.0139, 0.0128]],\n",
            "\n",
            "         [[0.0345, 0.0345, 0.0346, 0.0344, 0.0357],\n",
            "          [0.0337, 0.0368, 0.0320, 0.0339, 0.0340],\n",
            "          [0.0329, 0.0346, 0.0369, 0.0330, 0.0358],\n",
            "          [0.0348, 0.0344, 0.0357, 0.0333, 0.0347],\n",
            "          [0.0337, 0.0353, 0.0348, 0.0366, 0.0348]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
            "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 512, 220, 220]          38,912\n",
            "         MaxPool2d-2        [-1, 512, 110, 110]               0\n",
            "            Conv2d-3        [-1, 256, 106, 106]       3,277,056\n",
            "         MaxPool2d-4          [-1, 256, 53, 53]               0\n",
            "            Conv2d-5          [-1, 128, 52, 52]         131,200\n",
            "         MaxPool2d-6          [-1, 128, 26, 26]               0\n",
            "            Conv2d-7           [-1, 64, 24, 24]          73,792\n",
            "         MaxPool2d-8           [-1, 64, 12, 12]               0\n",
            "            Conv2d-9           [-1, 32, 10, 10]          18,464\n",
            "        MaxPool2d-10             [-1, 32, 5, 5]               0\n",
            "  ConvTranspose2d-11             [-1, 64, 7, 7]          18,496\n",
            "  ConvTranspose2d-12            [-1, 128, 9, 9]          73,856\n",
            "  ConvTranspose2d-13          [-1, 256, 10, 10]         131,328\n",
            "  ConvTranspose2d-14          [-1, 512, 14, 14]       3,277,312\n",
            "  ConvTranspose2d-15          [-1, 256, 18, 18]       3,277,056\n",
            "================================================================\n",
            "Total params: 10,317,472\n",
            "Trainable params: 10,317,472\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 269.14\n",
            "Params size (MB): 39.36\n",
            "Estimated Total Size (MB): 309.07\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ],
      "source": [
        "# final customized cnn model by calculating\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "y=15\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
        "        self.conv1 = nn.Conv2d(3, 512, 5, padding =0,stride =1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(512, 256, 5, padding =0,stride =1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv3 = nn.Conv2d(256, 128, 2, padding =0,stride =1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv4 = nn.Conv2d(128,64, 3, padding =0,stride =1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv5 = nn.Conv2d(64,32, 3, padding =0,stride =1)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.classifier = nn.Linear(64*32*3, 2)\n",
        "        classifier = self.classifier\n",
        "        print('classifier',classifier)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "         ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv1 = nn.ConvTranspose2d(32, 64, 3, padding=0, stride=1)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(64, 128, 3,padding=0, stride=1)\n",
        "        self.t_conv3 = nn.ConvTranspose2d(128, 256, 2,padding=0, stride=1)\n",
        "        self.t_conv4 = nn.ConvTranspose2d(256, 512, 5,padding=0, stride=1)\n",
        "        self.t_conv5 = nn.ConvTranspose2d(512, 256, 5,padding=0, stride=1)\n",
        "        \n",
        "        \n",
        "        #self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        '''self.conv2 = nn.Conv2d(64, 256, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        '''\n",
        "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "        \n",
        "        '''self.conv2 = nn.Conv2d(256, 512, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(512, 512, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(512, 512, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 512, 3, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 512, 3, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 256, 3, stride=2)\n",
        "        self.t_conv1 = nn.Conv2d(3, 64, 3, padding =0,stride =2)\n",
        "        self.conv1 = nn.Conv2d(64, 64, 3, padding =0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(64, 256, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        '''\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        # add hidden layers with relu activation function\n",
        "        # and maxpooling after\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        # add second hidden layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        # add third hidden layer\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        # add fouth hidden layer\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        # add 5th hidden layer\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.pool(x)# compressed representation\n",
        "        encoder = x;\n",
        "        output =nn.Flatten();\n",
        "        output =output(encoder)\n",
        "        global y\n",
        "        y=output\n",
        "\n",
        "        \n",
        "        print(x)\n",
        "        \n",
        "        ## decode ##\n",
        "        # add transpose conv layers, with relu activation function\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.relu(self.t_conv2(x))\n",
        "        x = F.relu(self.t_conv3(x))\n",
        "        x = F.relu(self.t_conv4(x))\n",
        "        # output layer (with sigmoid for scaling from 0 to 1)\n",
        "        x = F.sigmoid(self.t_conv5(x))\n",
        "                \n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = ConvAutoencoder()\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "summary(model,(3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)\n",
        "print('size',y.size())\n",
        "k=torch.flatten(y)\n",
        "#k=k(y)\n",
        "print(k)\n",
        "#k=2*800\n",
        "print(k)\n",
        "y.size()\n",
        "        "
      ],
      "metadata": {
        "id": "7QlG-kfUbc8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63a72a0-5695-4571-a157-f35419dbf247"
      },
      "id": "7QlG-kfUbc8c",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0120, 0.0118, 0.0113,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0122, 0.0119, 0.0115,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "size torch.Size([2, 800])\n",
            "tensor([0.0120, 0.0118, 0.0113,  ..., 0.0000, 0.0000, 0.0000],\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "tensor([0.0120, 0.0118, 0.0113,  ..., 0.0000, 0.0000, 0.0000],\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 800])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7265ac5",
      "metadata": {
        "id": "d7265ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "43215372-346e-4656-b052-2450cc42fdc4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0962591cd461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConvAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "#customized cnn model by calculating for 32 X32\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
        "        x = nn.Conv2d(3, 16,3 , padding =0,stride =1)\n",
        "        #x= nn.ReLU(x)\n",
        "        x = nn.MaxPool2d(2,2)\n",
        "        x = nn.Conv2d(16, 8, 3, padding =0,stride =1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv3 = nn.Conv2d(8, 4, 3, padding =0,stride =1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "         ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv1 = nn.ConvTranspose2d(32, 64, 3, padding=0, stride=1)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(64, 128, 3,padding=0, stride=1)\n",
        "        self.t_conv3 = nn.ConvTranspose2d(128, 256, 2,padding=0, stride=1)\n",
        "        self.t_conv4 = nn.ConvTranspose2d(256, 512, 5,padding=0, stride=1)\n",
        "        self.t_conv5 = nn.ConvTranspose2d(512, 256, 5,padding=0, stride=1)\n",
        "        \n",
        "        \n",
        "        #self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        '''self.conv2 = nn.Conv2d(64, 256, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        '''\n",
        "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "        \n",
        "        '''self.conv2 = nn.Conv2d(256, 512, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(512, 512, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(512, 512, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 512, 3, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 512, 3, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(512, 256, 3, stride=2)\n",
        "        self.t_conv1 = nn.Conv2d(3, 64, 3, padding =0,stride =2)\n",
        "        self.conv1 = nn.Conv2d(64, 64, 3, padding =0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(64, 256, 3, padding=0,stride =2)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=0,stride =2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        '''\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        # add hidden layers with relu activation function\n",
        "        # and maxpooling after\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        # add second hidden layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        # add third hidden layer\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        # add fouth hidden layer\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        # add 5th hidden layer\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.pool(x)# compressed representation\n",
        "        encoder = x;\n",
        "        \n",
        "        print(x)\n",
        "        \n",
        "        ## decode ##\n",
        "        # add transpose conv layers, with relu activation function\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.relu(self.t_conv2(x))\n",
        "        x = F.relu(self.t_conv3(x))\n",
        "        x = F.relu(self.t_conv4(x))\n",
        "        # output layer (with sigmoid for scaling from 0 to 1)\n",
        "        x = F.sigmoid(self.t_conv5(x))\n",
        "                \n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = ConvAutoencoder()\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "summary(model,(3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "import os\n",
        "\n",
        "print(os.listdir(\"images/\"))\n",
        "\n",
        "SIZE = 512 #Resize images\n",
        "\n",
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob(\"images/train_images\"):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        train_images.append(img)\n",
        "        #train_labels.append(label)\n",
        "        \n",
        "train_images = np.array(train_images)\n",
        "\n",
        "train_masks = [] \n",
        "for directory_path in glob.glob(\"images/train_masks\"):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        mask = cv2.imread(mask_path, 0)       \n",
        "        mask = cv2.resize(mask, (SIZE, SIZE))\n",
        "        #mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)\n",
        "        train_masks.append(mask)\n",
        "        #train_labels.append(label)\n",
        "        \n",
        "train_masks = np.array(train_masks)\n",
        "\n",
        "X_train = train_images\n",
        "y_train = train_masks\n",
        "y_train = np.expand_dims(y_train, axis=3)\n",
        "\n",
        "\n",
        "activation = 'sigmoid'\n",
        "feature_extractor = Sequential()\n",
        "feature_extractor.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (SIZE, SIZE, 3)))\n",
        "feature_extractor.add(Conv2D(32, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
        "\n",
        "#feature_extractor.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
        "#feature_extractor.add(BatchNormalization())\n",
        "#\n",
        "#feature_extractor.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
        "#feature_extractor.add(BatchNormalization())\n",
        "#feature_extractor.add(MaxPooling2D())\n",
        "#feature_extractor.add(Flatten())\n",
        "\n",
        "X = feature_extractor.predict(X_train)\n",
        "\n",
        "X = X.reshape(-1, X.shape[3])\n",
        "\n",
        "Y = y_train.reshape(-1)\n",
        "\n",
        "dataset = pd.DataFrame(X)\n",
        "dataset['Label'] = Y\n",
        "print(dataset['Label'].unique())\n",
        "print(dataset['Label'].value_counts())\n",
        "\n",
        "##If we do not want to include pixels with value 0 \n",
        "##e.g. Sometimes unlabeled pixels may be given a value 0.\n",
        "dataset = dataset[dataset['Label'] != 0]\n",
        "\n",
        "X_for_RF = dataset.drop(labels = ['Label'], axis=1)\n",
        "Y_for_RF = dataset['Label']\n",
        "\n",
        "#RANDOM FOREST\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n",
        "\n",
        "# Train the model on training data\n",
        "# Ravel Y to pass 1d array instead of column vector\n",
        "model.fit(X_for_RF, Y_for_RF) #For sklearn no one hot encoding\n",
        "\n",
        "\n",
        "filename = 'RF_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "\n",
        "\n",
        "#READ EXTERNAL IMAGE...\n",
        "test_img = cv2.imread('images/test_images/Sandstone_Versa0360.tif', cv2.IMREAD_COLOR)       \n",
        "test_img = cv2.resize(test_img, (SIZE, SIZE))\n",
        "test_img = cv2.cvtColor(test_img, cv2.COLOR_RGB2BGR)\n",
        "test_img = np.expand_dims(test_img, axis=0)\n",
        "\n",
        "#predict_image = np.expand_dims(X_train[8,:,:,:], axis=0)\n",
        "X_test_feature = feature_extractor.predict(test_img)\n",
        "X_test_feature = X_test_feature.reshape(-1, X_test_feature.shape[3])\n",
        "\n",
        "prediction = loaded_model.predict(X_test_feature)\n",
        "\n",
        "prediction_image = prediction.reshape(mask.shape)\n",
        "plt.imshow(prediction_image, cmap='gray')\n"
      ],
      "metadata": {
        "id": "nLc4rNgKGrcg"
      },
      "id": "nLc4rNgKGrcg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677f8d20",
      "metadata": {
        "id": "677f8d20"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c2e6fb",
      "metadata": {
        "id": "01c2e6fb"
      },
      "outputs": [],
      "source": [
        "X_train_encode = encoder.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbbb4210",
      "metadata": {
        "id": "dbbb4210"
      },
      "outputs": [],
      "source": [
        "#tensorflow based modified simple auto encoder\n",
        "\n",
        "#Importing the Libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f673b656",
      "metadata": {
        "id": "f673b656",
        "outputId": "1619a71e-c777-46d2-e888-9a67b2a6b510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 100) (1000,)\n"
          ]
        }
      ],
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c43d6202",
      "metadata": {
        "id": "c43d6202"
      },
      "outputs": [],
      "source": [
        "# split into train test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa8657c",
      "metadata": {
        "id": "aaa8657c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5221413f-5422-4073-e935-3a920aa24f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name=None), name='leaky_re_lu_1/LeakyRelu:0', description=\"created by layer 'leaky_re_lu_1'\")\n"
          ]
        }
      ],
      "source": [
        "# AutoEncoder Model Preparation\n",
        "n_inputs = X.shape[1]\n",
        "# define encoder\n",
        "input_data_shape= Input(shape=(n_inputs,))\n",
        "# encoder level 1\n",
        "encoder= Dense(n_inputs*2)(input_data_shape)\n",
        "encoder = BatchNormalization()(encoder)\n",
        "encoder= LeakyReLU()(encoder)\n",
        "# encoder level 2\n",
        "encoder= Dense(n_inputs)(encoder)\n",
        "encoder= BatchNormalization()(encoder)\n",
        "encoder= LeakyReLU()(encoder)\n",
        "print(encoder)\n",
        "# bottleneck\n",
        "n_bottleneck = round(float(n_inputs) / 2.0)\n",
        "bottleneck = Dense(n_bottleneck)(encoder)\n",
        "# define decoder, level 1\n",
        "decoder = Dense(n_inputs)(bottleneck)\n",
        "decoder = BatchNormalization()(decoder)\n",
        "decoder = LeakyReLU()(decoder)\n",
        "# decoder level 2\n",
        "decoder = Dense(n_inputs*2)(decoder)\n",
        "decoder = BatchNormalization()(decoder)\n",
        "decoder = LeakyReLU()(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0d96ef",
      "metadata": {
        "id": "da0d96ef"
      },
      "outputs": [],
      "source": [
        "# output layer\n",
        "output = Dense(n_inputs, activation='linear')(decoder)\n",
        "# define autoencoder model\n",
        "model = Model(inputs=input_data_shape, outputs=output)\n",
        "# compile autoencoder model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948192e3",
      "metadata": {
        "id": "948192e3"
      },
      "outputs": [],
      "source": [
        "# fit the autoencoder model to reconstruct input\n",
        "# fit the autoencoder model to reconstruct input\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=2, validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e2fe34",
      "metadata": {
        "id": "88e2fe34"
      },
      "outputs": [],
      "source": [
        "# define an encoder model (without the decoder)\n",
        "encoder = Model(inputs=input_data_shape, outputs=bottleneck)\n",
        "# save the encoder to file\n",
        "encoder.save('encoder.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b3daf7",
      "metadata": {
        "id": "45b3daf7"
      },
      "outputs": [],
      "source": [
        "#Building a Base Model to compare the performance after compressing the data using Encoder model.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)\n",
        "# define model\n",
        "model = LogisticRegression()\n",
        "# fit model on training set\n",
        "model.fit(X_train, y_train)\n",
        "# make prediction on test set\n",
        "yhat = model.predict(X_test)\n",
        "# calculate accuracy\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b874bba",
      "metadata": {
        "id": "6b874bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "45c4a323-52ef-4c09-d4f2-274c160ac8e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d60521abdae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Compressing the input data using Encoder Model and fitting it on the Logistic Regression model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# load the model from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoder.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# encode the train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at encoder.h5"
          ]
        }
      ],
      "source": [
        "#Compressing the input data using Encoder Model and fitting it on the Logistic Regression model.\n",
        "# load the model from file\n",
        "encoder = load_model('encoder.h5')\n",
        "\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "# fit the model on the training set\n",
        "model.fit(X_train_encode, y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53098c1f",
      "metadata": {
        "id": "53098c1f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8508db8a",
      "metadata": {
        "id": "8508db8a"
      },
      "source": [
        "# tutorial for cifer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70199c3f",
      "metadata": {
        "id": "70199c3f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters \n",
        "num_epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# dataset has PILImage images of range [0, 1]. \n",
        "# We transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 512, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -> n, 3, 32, 32\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
        "        x = F.relu(self.fc1(x))               # -> n, 120\n",
        "        x = F.relu(self.fc2(x))               # -> n, 84\n",
        "        x = self.fc3(x)                       # -> n, 10\n",
        "        return x\n",
        "\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1afd64dd",
      "metadata": {
        "id": "1afd64dd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "customizedautoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}